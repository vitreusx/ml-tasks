{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbX1SWcmVjjM"
   },
   "source": [
    "# Reproduciblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "sep = ', '\n",
    "def Dataset(path):\n",
    "    dataset_json = json.load(open(path, 'r'))\n",
    "    \n",
    "    return pd.DataFrame(data=[{\n",
    "        'id': record['id'],\n",
    "        'X': sep.join(record['ingredients']),\n",
    "        'y': record.get('cuisine', '')\n",
    "    } for record in dataset_json])\n",
    "\n",
    "def train_ds():\n",
    "    return Dataset('data/train.json')\n",
    "\n",
    "def test_ds():\n",
    "    return Dataset('data/test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model (`hybrid`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inasfar as the model itself is concerned, we will first vectorize the ingredients into sparse features, then use a combination of SVD and \"sparse\" models for each cuisine class to construct a dense representation, and then train a \"dense\" model on this representation. I hope this doesn't qualify as \"a primitive heurestic based on single ingredients\" that we were supposed to avoid.\n",
    "\n",
    "Other than that, there were few other reasons behind this particular model:\n",
    "- I principally wanted to experiment with \"multi-stage\" models;\n",
    "- I wanted it to be trainable on my laptop without a GPU;\n",
    "- I wanted to see whether such a (comparatively) simple model would yield good results.\n",
    "\n",
    "I will also observe that I couldn't think of any _useful_ visualisation or a graph that could be used for this task; the extent of the \"aesthetic\" features in this notebook are the progress bars (using the library `tqdm`).\n",
    "\n",
    "The score on Kaggle was 0.79877, not sure whether it's bad or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredient vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we need to numerically represent the text; for that purpose, we will extract (lemmatized) words and related tokens from the ingredients' lists and take (1-10)-grams of them as the features. This uses an \"external source\" (`nltk` and `wordnet` for lemmatization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/talos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def concat(xs):\n",
    "    return {x for lst in xs for x in lst}\n",
    "\n",
    "class Analyzer:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def analyze_one(self, ingredient):\n",
    "        words = ingredient.lower().split(' ')     \n",
    "        lemmatized = [self.lemmatizer.lemmatize(word)\n",
    "                      for word in words]\n",
    "        grams = {' '.join(gram)\n",
    "                 for n in range(1, 10)\n",
    "                 for gram in ngrams(lemmatized, n)}\n",
    "        return {*grams, ingredient}\n",
    "    \n",
    "    def __call__(self, ingredients_str):\n",
    "        ingredients = ingredients_str.split(sep)\n",
    "        tokens = concat(self.analyze_one(ingredient)\n",
    "                         for ingredient in ingredients)\n",
    "        return list(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def Vectorizer():\n",
    "    return CountVectorizer(analyzer=Analyzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a number of helper functions to (hyper)fit the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def bayes_cv(est, search_spaces, **extras):\n",
    "    return BayesSearchCV(est, search_spaces, **{\n",
    "        'refit': True, 'verbose': 0, 'n_jobs': -1, 'cv': 5,\n",
    "        'random_state': seed, **extras\n",
    "    })\n",
    "\n",
    "def nest(prefix, search_spaces):\n",
    "    return [({'{}__{}'.format(prefix, name): value\n",
    "              for name, value in grid.items()}, n_iter)\n",
    "            for grid, n_iter in search_spaces]\n",
    "\n",
    "def hyperfit(desc, cv, X, y=None):\n",
    "    bar = iter(tqdm(range(cv.total_iterations),\n",
    "                    desc=desc))\n",
    "    \n",
    "    def cb(_):\n",
    "        nonlocal bar\n",
    "        next(bar)\n",
    "        \n",
    "    cv.fit(X, y, callback=cb)\n",
    "    for _ in bar: pass\n",
    "    \n",
    "    print('Score: {}'.format(cv.best_score_))\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A repository of sorts for the hyperparameter ranges and base models. Most of them won't be used, but I wanted to keep the hyperparameters around regardless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "def SparseLR(n_iter=100):\n",
    "    est = LogisticRegression(random_state=seed,\n",
    "                             penalty='l2',\n",
    "                             solver='liblinear')\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'C': (1e-4, 1e4, 'log-uniform'),\n",
    "    }, n_iter)], scoring='roc_auc')\n",
    "\n",
    "def SparseLSVC(n_iter=75):\n",
    "    est = LinearSVC(random_state=seed, penalty='l1', solver='hinge', dual=False)\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'C': (1e-4, 1e4, 'log-uniform'),\n",
    "    }, n_iter)], scoring='roc_auc')\n",
    "\n",
    "def SparseLGBM(n_iter=50):\n",
    "    est = LGBMClassifier(random_state=seed)\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'boosting_type': ['dart', 'gbdt'],\n",
    "        'num_leaves': np.geomspace(15, 128, 10, dtype=int),\n",
    "        'learning_rate': (0.005, 0.75, 'log-uniform'),\n",
    "        'n_estimators': np.geomspace(100, 1000, 25, dtype=int),\n",
    "        'colsample_bytree': (0.5, 1, 'uniform'),\n",
    "        'min_child_weight': (1e-4, 1e-1, 'log-uniform'),\n",
    "        'min_child_samples': np.geomspace(20, 250, 10, dtype=int),\n",
    "        'subsample': (0.5, 1, 'uniform')\n",
    "    }, n_iter)], scoring='roc_auc')\n",
    "\n",
    "def DenseLGBM(n_iter=10):\n",
    "    est = LGBMClassifier(random_state=seed)\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'boosting_type': ['dart', 'gbdt'],\n",
    "        'num_leaves': np.geomspace(15, 128, 10, dtype=int),\n",
    "        'learning_rate': (0.005, 0.75, 'log-uniform'),\n",
    "        'n_estimators': np.geomspace(100, 300, 25, dtype=int),\n",
    "        'colsample_bytree': (0.5, 1, 'uniform'),\n",
    "        'min_child_weight': (1e-4, 1e-1, 'log-uniform'),\n",
    "        'min_child_samples': np.geomspace(20, 250, 10, dtype=int),\n",
    "        'subsample': (0.5, 1, 'uniform')\n",
    "    }, n_iter)], scoring='accuracy')\n",
    "\n",
    "def DenseXGB(n_iter=10):\n",
    "    est = XGBClassifier(random_state=seed, nthread=-1, objective='multi:softprob',\n",
    "                        eval_metric='logloss', use_label_encoder=False, booster='dart')\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'n_estimators': np.geomspace(100, 300, 25, dtype=int),\n",
    "        'learning_rate': (0.05, 1, 'log-uniform'),\n",
    "        'subsample': (0.5, 1, 'uniform'),\n",
    "        'max_depth': np.geomspace(2, 8, 4, dtype=int),\n",
    "        'colsample_bytree': (0.5, 1, 'uniform'),\n",
    "        'min_child_weight': (1, 5)\n",
    "    }, n_iter)], scoring='accuracy')\n",
    "\n",
    "def DenseRT(n_iter=10):\n",
    "    est = RandomForestClassifier(random_state=seed)\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'n_estimators': np.geomspace(100, 300, 25, dtype=int),\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'min_samples_split': np.geomspace(1, 50, 8, dtype=int),\n",
    "        'min_samples_leaf': np.geomspace(1, 50, 8, dtype=int)\n",
    "    }, n_iter)], scoring='accuracy')\n",
    "\n",
    "def DenseET(n_iter=10):\n",
    "    est = ExtraTreesClassifier(random_state=seed)\n",
    "    \n",
    "    return bayes_cv(est, [({\n",
    "        'n_estimators': np.geomspace(100, 300, 25, dtype=int),\n",
    "        'max_features': ['auto', 'sqrt', 'log2'],\n",
    "        'min_samples_split': np.geomspace(1, 50, 8, dtype=int),\n",
    "        'min_samples_leaf': np.geomspace(1, 50, 8, dtype=int)\n",
    "    }, n_iter)], scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word of note: I decided to backup the models and CV results for further analysis, but they weigh quite a lot so I don't attach them. Also, 100 iterations for the hyperparameter search may be an overkill, especially since there's essentially only the penalty parameter `C`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_model(shelf, label, prep_X, y=None):\n",
    "    prep_y = y.apply(lambda x: int(x == label))\n",
    "    entry = 'sparse/{}/lr'.format(label)\n",
    "    if entry not in shelf:\n",
    "        model_cv = hyperfit(entry, SparseLR(), prep_X, prep_y)\n",
    "        shelf[entry] = model_cv\n",
    "    else:\n",
    "        model_cv = shelf[entry]\n",
    "    \n",
    "    return model_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class SparseReducer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, code, model):\n",
    "        self.code = code\n",
    "        self.model = model\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        yt = np.where(y == self.code, 1, 0)\n",
    "        self.model.fit(X, yt)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        for method in ['predict_proba', 'decision_function', 'predict']:\n",
    "            if hasattr(self.model, method):\n",
    "                Xt = getattr(self.model, method)(X)\n",
    "                if len(Xt.shape) == 1:\n",
    "                    Xt = Xt.reshape((Xt.shape[0], 1))\n",
    "                return Xt\n",
    "\n",
    "def DenseHead(reducers):\n",
    "    base = DenseLGBM()\n",
    "    est = Pipeline([\n",
    "        ('reducer', FeatureUnion(reducers, n_jobs=-1)),\n",
    "        ('dense', base.estimator)\n",
    "    ])\n",
    "    grid = nest('dense', base.search_spaces)\n",
    "    \n",
    "    return bayes_cv(est, grid, scoring='accuracy')\n",
    "            \n",
    "class DenseModel:\n",
    "    def __init__(self, shelf):\n",
    "        self.prep = Pipeline([\n",
    "            ('vect', Vectorizer()),\n",
    "            ('tfidf', TfidfTransformer())\n",
    "        ])\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.shelf = shelf\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        prep_X = self.prep.fit_transform(X, y)\n",
    "        prep_y = self.encoder.fit_transform(y)\n",
    "        \n",
    "        reducers = [('svd', TruncatedSVD(n_components=60))]\n",
    "        for code, label in tqdm(list(enumerate(self.encoder.classes_)), desc='Sparse models'):\n",
    "            model = sparse_model(self.shelf, label, prep_X, y)\n",
    "            reducer = SparseReducer(code, model)\n",
    "            reducers.append((label, reducer))\n",
    "        \n",
    "        if 'dense' not in self.shelf:\n",
    "            final_cv = hyperfit('dense', DenseHead(reducers), prep_X, prep_y)\n",
    "            self.shelf['dense'] = final_cv\n",
    "        else:\n",
    "            final_cv = self.shelf['dense']\n",
    "        \n",
    "        self.final = final_cv.best_estimator_\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        prep_X = self.prep.transform(X)\n",
    "        predictions = self.final.predict(prep_X)\n",
    "        return self.encoder.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import shelve\n",
    "\n",
    "def main(uid=None, clear=False):\n",
    "    if uid is None:\n",
    "        uid = str(uuid.uuid4())\n",
    "    print('UID: {}'.format(uid))\n",
    "    \n",
    "    with shelve.open('results/{}.shelf'.format(uid), writeback=True) as shelf:\n",
    "        if clear:\n",
    "            shelf.clear()\n",
    "        \n",
    "        if 'train' not in shelf:\n",
    "            train = train_ds().sample(frac=1)\n",
    "            shelf['train'] = train\n",
    "        else:\n",
    "            train = shelf['train']\n",
    "            \n",
    "        if 'test' not in shelf:\n",
    "            test = test_ds()\n",
    "            shelf['test'] = test\n",
    "        else:\n",
    "            test = shelf['test']\n",
    "\n",
    "        model = DenseModel(shelf)\n",
    "        model.fit(train.X, train.y)\n",
    "        test['cuisine'] = model.predict(test.X)\n",
    "        test[['id', 'cuisine']].to_csv('results/{}.csv'.format(uid),\n",
    "                                       index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UID: hybrid\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227ffdd5769a4f52861edd2f2d03766e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sparse models:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139ee8f0a51d4d8e88de699a4efeed7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dense:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7974053401719716\n"
     ]
    }
   ],
   "source": [
    "main(uid='hybrid', clear=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCLeFDSOqerZnREwLDiMS5",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
