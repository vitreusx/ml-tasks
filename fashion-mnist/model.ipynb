{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def reset_seed():\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "reset_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I am using the description over at http://yann.lecun.com/exdb/mnist/, and assume that it's the same for Fashion MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-1d42f9b8bd0f>:14: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array([Image.frombytes('L', (rows, cols), file.read(rows * cols))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class FashionMNIST:\n",
    "    def load_images(self, fpath):\n",
    "        file = open(fpath, 'rb')\n",
    "        \n",
    "        file.read(4) # magic number\n",
    "        nimages = int.from_bytes(file.read(4), 'big')\n",
    "        rows = int.from_bytes(file.read(4), 'big')\n",
    "        cols = int.from_bytes(file.read(4),'big')\n",
    "        \n",
    "        return np.array([Image.frombytes('L', (rows, cols), file.read(rows * cols))\n",
    "                         for n in range(nimages)], dtype=object)\n",
    "    \n",
    "    def load_labels(self, fpath):\n",
    "        file = open(fpath, 'rb')\n",
    "        \n",
    "        file.read(4) # magic number\n",
    "        nlabels = int.from_bytes(file.read(4),'big')\n",
    "        labels = file.read(nlabels)\n",
    "        \n",
    "        return np.frombuffer(labels, dtype=np.uint8).astype(np.int64)\n",
    "    \n",
    "    def __init__(self, images, labels=None):\n",
    "        self.images = self.load_images(images)\n",
    "        if labels is not None:\n",
    "            self.labels = self.load_labels(labels)\n",
    "\n",
    "class TransDataset(Dataset):\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x = self.X[ix]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        \n",
    "        if self.y is not None:\n",
    "            return x, self.y[ix]\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "def load_mnist():\n",
    "    root = Path('data', 'FashionMNIST', 'raw')\n",
    "    train_images = Path(root, 'train-images-idx3-ubyte')\n",
    "    train_labels = Path(root, 'train-labels-idx1-ubyte')\n",
    "    test_images = Path(root, 't10k-images-idx3-ubyte')\n",
    "    \n",
    "    train_mnist = FashionMNIST(train_images, train_labels)\n",
    "    test_mnist = FashionMNIST(test_images)\n",
    "    \n",
    "    return {'train': train_mnist, 'test': test_mnist}\n",
    "\n",
    "mnist = load_mnist()    \n",
    "    \n",
    "def make_datasets(mnist, transforms=None, total_frac=1, val_frac=0.1):\n",
    "    if transforms:\n",
    "        train_trans = transforms['train']\n",
    "        val_trans = transforms['val']\n",
    "    else:\n",
    "        train_trans = None\n",
    "        val_trans = None\n",
    "    \n",
    "    nsamples = mnist['train'].images.shape[0]\n",
    "    ntotal = int(np.floor(total_frac*nsamples))\n",
    "    total_Ix = np.random.choice(nsamples, ntotal)\n",
    "    nval = int(np.floor(val_frac*len(total_Ix)))\n",
    "    val_Ix = total_Ix[np.random.choice(len(total_Ix), nval)]\n",
    "    train_Ix = np.setdiff1d(total_Ix, val_Ix)\n",
    "    \n",
    "    train_images = mnist['train'].images[train_Ix]\n",
    "    train_labels = mnist['train'].labels[train_Ix]\n",
    "    val_images = mnist['train'].images[val_Ix]\n",
    "    val_labels = mnist['train'].labels[val_Ix]\n",
    "    test_images = mnist['test'].images\n",
    "    \n",
    "    train_ds = TransDataset(train_images, train_labels, transform=train_trans)\n",
    "    val_ds = TransDataset(val_images, val_labels, transform=val_trans)\n",
    "    test_ds = TransDataset(test_images, transform=val_trans)\n",
    "    \n",
    "    return {'train': train_ds, 'val': val_ds, 'test': test_ds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the unagumented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d70dba5e834ba2a9e72607e8897d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_ds = make_datasets(mnist)['train']\n",
    "\n",
    "nexamples = 5\n",
    "classes = np.unique(examples_ds.y)\n",
    "\n",
    "fig, axes = plt.subplots(len(classes), nexamples)\n",
    "fig.tight_layout()\n",
    "\n",
    "for i, class_ in enumerate(classes):\n",
    "    images_Ix = np.where(examples_ds.y == class_)[0]\n",
    "    samples_Ix = images_Ix[np.random.choice(len(images_Ix), nexamples)]\n",
    "    for j, sample_Ix in enumerate(samples_Ix):\n",
    "        axes[i][j].imshow(examples_ds.X[sample_Ix], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inasfar as data augmentation is concerned, we will upscale (crops of) images to 128x128, apply random flips and rotations, and also random erasing. This *should* make our models more robust (of course the best strategy is to manually check what works and what doesn't)\n",
    "\n",
    "For validation images (and test images for final results), we will only resize the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "datasets = make_datasets(mnist, transforms={\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(64, scale=(0.75, 1)),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "#         transforms.RandomRotation(90),\n",
    "#         transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(),\n",
    "        transforms.Normalize((0.5), (0.5)),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "#         transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))\n",
    "    ])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these augmentations, the images are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e766b2285b4570b6a71950a067d6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples_Ix = np.random.choice(len(datasets['train']), 10)\n",
    "fig, axes = plt.subplots(2, 5)\n",
    "axes_Ix = [(i, j) for i in range(2) for j in range(5)]\n",
    "\n",
    "for (i, j), example_ix in zip(axes_Ix, examples_Ix):\n",
    "    axes[i][j].imshow(datasets['train'][example_ix][0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downscaling from 64x64 to 32x32 should not (in my estimation) impact the accuracy that much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start without ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18, reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, I decided to look at ResNet18 implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models import resnet18\n",
    "resnet = resnet18()\n",
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make a number of adjustments:\n",
    "- the number of channels in input (`conv1`) layer should be 1 instead of 3;\n",
    "- the output (`fc`) layer should have 10 outputs (as there are 10 classes in out dataset), as opposed to 1000 (from ImageNet).\n",
    "\n",
    "I would also conjecture that we do not really need *that many* channels to get the accuracy we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Conv2d, Linear\n",
    "resnet.conv1 = Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "resnet.fc = Linear(in_features=512, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple (i.e. custom) CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also test some custom-defined models, usually modeled on the first few layers of ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 5, 1, 2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.bn1 = nn.BatchNorm2d(16, eps=1e-05, momentum=0.1,\n",
    "                                  affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, 1, 2)\n",
    "        self.bn2 = nn.BatchNorm2d(32, eps=1e-05, momentum=0.1,\n",
    "                                  affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, 5, 1, 2)\n",
    "        self.bn3 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1,\n",
    "                                  affine=True, track_running_stats=True)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 14 * 14, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.pool(func.relu(x))\n",
    "        \n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = func.relu(x)\n",
    "        \n",
    "        x = self.bn3(self.conv3(x))\n",
    "        \n",
    "        x = x.view(-1, 64 * 14 * 14)\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define data loaders and some other things related to Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(datasets['train'], batch_size=4,\n",
    "                        shuffle=True, num_workers=4),\n",
    "    'val': DataLoader(datasets['val'], batch_size=4,\n",
    "                      shuffle=True, num_workers=4),\n",
    "    'test': DataLoader(datasets['test'], batch_size=4,\n",
    "                       num_workers=4),\n",
    "}\n",
    "dataset_sizes = {name: len(ds) for name, ds in datasets.items()}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the training code. (Taken from https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html, I'm not sure whether that's \"cheating\", but I didn't exactly think of any reason to modify it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "def report_time(since):\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Elapsed: {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        report_time(since)\n",
    "        print()\n",
    "\n",
    "    report_time(since)\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "----------\n",
      "train Loss: 0.5026 Acc: 0.8156\n",
      "val Loss: 0.3495 Acc: 0.8703\n",
      "Elapsed: 3m 29s\n",
      "\n",
      "Epoch 2/25\n",
      "----------\n",
      "train Loss: 0.3441 Acc: 0.8734\n",
      "val Loss: 0.3027 Acc: 0.8858\n",
      "Elapsed: 6m 48s\n",
      "\n",
      "Epoch 3/25\n",
      "----------\n",
      "train Loss: 0.2978 Acc: 0.8909\n",
      "val Loss: 0.2886 Acc: 0.8927\n",
      "Elapsed: 10m 14s\n",
      "\n",
      "Epoch 4/25\n",
      "----------\n",
      "train Loss: 0.2646 Acc: 0.9022\n",
      "val Loss: 0.2762 Acc: 0.9042\n",
      "Elapsed: 13m 37s\n",
      "\n",
      "Epoch 5/25\n",
      "----------\n",
      "train Loss: 0.2389 Acc: 0.9084\n",
      "val Loss: 0.2818 Acc: 0.9035\n",
      "Elapsed: 16m 50s\n",
      "\n",
      "Epoch 6/25\n",
      "----------\n",
      "train Loss: 0.2188 Acc: 0.9180\n",
      "val Loss: 0.2619 Acc: 0.9068\n",
      "Elapsed: 20m 7s\n",
      "\n",
      "Epoch 7/25\n",
      "----------\n",
      "train Loss: 0.2014 Acc: 0.9240\n",
      "val Loss: 0.2629 Acc: 0.9128\n",
      "Elapsed: 23m 33s\n",
      "\n",
      "Epoch 8/25\n",
      "----------\n",
      "train Loss: 0.1351 Acc: 0.9510\n",
      "val Loss: 0.2517 Acc: 0.9195\n",
      "Elapsed: 26m 56s\n",
      "\n",
      "Epoch 9/25\n",
      "----------\n",
      "train Loss: 0.1149 Acc: 0.9561\n",
      "val Loss: 0.2460 Acc: 0.9232\n",
      "Elapsed: 30m 8s\n",
      "\n",
      "Epoch 10/25\n",
      "----------\n",
      "train Loss: 0.1102 Acc: 0.9592\n",
      "val Loss: 0.2519 Acc: 0.9218\n",
      "Elapsed: 33m 12s\n",
      "\n",
      "Epoch 11/25\n",
      "----------\n",
      "train Loss: 0.1020 Acc: 0.9617\n",
      "val Loss: 0.2544 Acc: 0.9222\n",
      "Elapsed: 36m 15s\n",
      "\n",
      "Epoch 12/25\n",
      "----------\n",
      "train Loss: 0.0986 Acc: 0.9633\n",
      "val Loss: 0.2562 Acc: 0.9215\n",
      "Elapsed: 39m 20s\n",
      "\n",
      "Epoch 13/25\n",
      "----------\n",
      "train Loss: 0.0950 Acc: 0.9655\n",
      "val Loss: 0.2542 Acc: 0.9255\n",
      "Elapsed: 42m 24s\n",
      "\n",
      "Epoch 14/25\n",
      "----------\n",
      "train Loss: 0.0899 Acc: 0.9671\n",
      "val Loss: 0.2620 Acc: 0.9190\n",
      "Elapsed: 45m 39s\n",
      "\n",
      "Epoch 15/25\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9697\n",
      "val Loss: 0.2559 Acc: 0.9245\n",
      "Elapsed: 49m 14s\n",
      "\n",
      "Epoch 16/25\n",
      "----------\n",
      "train Loss: 0.0836 Acc: 0.9692\n",
      "val Loss: 0.2592 Acc: 0.9247\n",
      "Elapsed: 52m 42s\n",
      "\n",
      "Epoch 17/25\n",
      "----------\n",
      "train Loss: 0.0807 Acc: 0.9707\n",
      "val Loss: 0.2604 Acc: 0.9247\n",
      "Elapsed: 55m 46s\n",
      "\n",
      "Epoch 18/25\n",
      "----------\n",
      "train Loss: 0.0833 Acc: 0.9700\n",
      "val Loss: 0.2574 Acc: 0.9250\n",
      "Elapsed: 58m 51s\n",
      "\n",
      "Epoch 19/25\n",
      "----------\n",
      "train Loss: 0.0805 Acc: 0.9710\n",
      "val Loss: 0.2607 Acc: 0.9235\n",
      "Elapsed: 62m 12s\n",
      "\n",
      "Epoch 20/25\n",
      "----------\n",
      "train Loss: 0.0810 Acc: 0.9715\n",
      "val Loss: 0.2594 Acc: 0.9242\n",
      "Elapsed: 65m 18s\n",
      "\n",
      "Epoch 21/25\n",
      "----------\n",
      "train Loss: 0.0781 Acc: 0.9724\n",
      "val Loss: 0.2616 Acc: 0.9243\n",
      "Elapsed: 68m 32s\n",
      "\n",
      "Epoch 22/25\n",
      "----------\n",
      "train Loss: 0.0795 Acc: 0.9712\n",
      "val Loss: 0.2581 Acc: 0.9237\n",
      "Elapsed: 71m 42s\n",
      "\n",
      "Epoch 23/25\n",
      "----------\n",
      "train Loss: 0.0786 Acc: 0.9708\n",
      "val Loss: 0.2632 Acc: 0.9228\n",
      "Elapsed: 75m 7s\n",
      "\n",
      "Epoch 24/25\n",
      "----------\n",
      "train Loss: 0.0811 Acc: 0.9709\n",
      "val Loss: 0.2622 Acc: 0.9235\n",
      "Elapsed: 78m 33s\n",
      "\n",
      "Epoch 25/25\n",
      "----------\n",
      "train Loss: 0.0755 Acc: 0.9728\n",
      "val Loss: 0.2631 Acc: 0.9237\n",
      "Elapsed: 81m 51s\n",
      "\n",
      "Elapsed: 81m 51s\n",
      "Best val Acc: 0.925500\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import lr_scheduler, SGD, AdamW\n",
    "\n",
    "model = SimpleNet()\n",
    "model.to(device)\n",
    "\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer = AdamW(model.parameters())\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, scheduler, num_epochs=25)\n",
    "torch.save(model.state_dict(), Path('models', 'simple.adam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may reload a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SimpleNet()\n",
    "# model.load_state_dict(torch.load(Path('models', 'simple2')))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(columns=['Id', 'Class'])\n",
    "\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    for images in dataloaders['test']:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        for pred in predicted.numpy():\n",
    "            preds = preds.append({'Id': idx, 'Class': pred},\n",
    "                                 ignore_index=True)\n",
    "            idx += 1\n",
    "\n",
    "preds.to_csv(Path('preds', 'simple.sgd.csv'),\n",
    "             index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
